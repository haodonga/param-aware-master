wandb: Currently logged in as: 910877690 (ycx). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /workspace/Edge_popup/hidden-networks-master/wandb/run-20241014_140121-e1omhfek
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run open_finetune_1e-06_unif_0.5_CIFAR10_cResNet18_512_1_test
wandb: â­ï¸ View project at https://wandb.ai/ycx/EP%E6%94%B9%E9%80%A0%E8%AE%A1%E5%88%92
wandb: ðŸš€ View run at https://wandb.ai/ycx/EP%E6%94%B9%E9%80%A0%E8%AE%A1%E5%88%92/runs/e1omhfek
wandb: logging graph, to disable use `wandb.watch(log_graph=False)`
=> Reading YAML config from configs/smallscale/resnet18/resnet18-usc-unsigned.yaml
Namespace(arch='cResNet18', batch_size=512, bn_type='NonAffineBatchNorm', config='configs/smallscale/resnet18/resnet18-usc-unsigned.yaml', conv_type='SubnetConv', data='dataset', epochs=1, evaluate=False, finetune=True, finetune_aftertrain=False, first_layer_dense=False, first_layer_type=None, freeze_weights=True, init='signed_constant', label_smoothing=None, last_layer_dense=False, log_dir=None, low_data=1, lr=0.1, lr_policy='cosine_lr', mode='fan_in', momentum=0.9, multigpu=[1], multistep_lr_adjust=30, multistep_lr_gamma=0.1, name='test', nesterov=False, no_bn_decay=False, nonlinearity='relu', num_classes=10, one_batch=False, optimizer='sgd', pretrained=None, print_freq=10, prune_rate=0.5, random_subnet=False, resume='', save_every=-1, scale_fan=False, score_init_constant=None, scores_lambda=1e-06, seed=None, set='CIFAR10', start_epoch=None, trainer='default', warmup_length=0, weight_decay=0.0005, width_mult=1.0, workers=4)
=> Using trainer from trainers.default
=> Creating model 'cResNet18'
==> Conv Type: SubnetConv
==> BN Type: NonAffineBatchNorm
==> Building first layer with <class 'utils.conv_type.SubnetConv'>
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
æƒé‡scåˆå§‹åŒ–
==> Setting prune rate of network to 0.5
==> Setting prune rate of conv1 to 0.5
==> Setting prune rate of layer1.0.conv1 to 0.5
==> Setting prune rate of layer1.0.conv2 to 0.5
==> Setting prune rate of layer1.1.conv1 to 0.5
==> Setting prune rate of layer1.1.conv2 to 0.5
==> Setting prune rate of layer2.0.conv1 to 0.5
==> Setting prune rate of layer2.0.conv2 to 0.5
==> Setting prune rate of layer2.0.shortcut.0 to 0.5
==> Setting prune rate of layer2.1.conv1 to 0.5
==> Setting prune rate of layer2.1.conv2 to 0.5
==> Setting prune rate of layer3.0.conv1 to 0.5
==> Setting prune rate of layer3.0.conv2 to 0.5
==> Setting prune rate of layer3.0.shortcut.0 to 0.5
==> Setting prune rate of layer3.1.conv1 to 0.5
==> Setting prune rate of layer3.1.conv2 to 0.5
==> Setting prune rate of layer4.0.conv1 to 0.5
==> Setting prune rate of layer4.0.conv2 to 0.5
==> Setting prune rate of layer4.0.shortcut.0 to 0.5
==> Setting prune rate of layer4.1.conv1 to 0.5
==> Setting prune rate of layer4.1.conv2 to 0.5
==> Setting prune rate of fc to 0.5
=> Rough estimate model params 5582176
=> Freezing model weights
==> No gradient to conv1.weight
==> No gradient to layer1.0.conv1.weight
==> No gradient to layer1.0.conv2.weight
==> No gradient to layer1.1.conv1.weight
==> No gradient to layer1.1.conv2.weight
==> No gradient to layer2.0.conv1.weight
==> No gradient to layer2.0.conv2.weight
==> No gradient to layer2.0.shortcut.0.weight
==> No gradient to layer2.1.conv1.weight
==> No gradient to layer2.1.conv2.weight
==> No gradient to layer3.0.conv1.weight
==> No gradient to layer3.0.conv2.weight
==> No gradient to layer3.0.shortcut.0.weight
==> No gradient to layer3.1.conv1.weight
==> No gradient to layer3.1.conv2.weight
==> No gradient to layer4.0.conv1.weight
==> No gradient to layer4.0.conv2.weight
==> No gradient to layer4.0.shortcut.0.weight
==> No gradient to layer4.1.conv1.weight
==> No gradient to layer4.1.conv2.weight
==> No gradient to fc.weight
=> Parallelizing on [1] gpus
<DEBUG> no gradient to module.conv1.weight
<DEBUG> gradient to module.conv1.scores
<DEBUG> no gradient to module.layer1.0.conv1.weight
<DEBUG> gradient to module.layer1.0.conv1.scores
<DEBUG> no gradient to module.layer1.0.conv2.weight
<DEBUG> gradient to module.layer1.0.conv2.scores
<DEBUG> no gradient to module.layer1.1.conv1.weight
<DEBUG> gradient to module.layer1.1.conv1.scores
<DEBUG> no gradient to module.layer1.1.conv2.weight
<DEBUG> gradient to module.layer1.1.conv2.scores
<DEBUG> no gradient to module.layer2.0.conv1.weight
<DEBUG> gradient to module.layer2.0.conv1.scores
<DEBUG> no gradient to module.layer2.0.conv2.weight
<DEBUG> gradient to module.layer2.0.conv2.scores
<DEBUG> no gradient to module.layer2.0.shortcut.0.weight
<DEBUG> gradient to module.layer2.0.shortcut.0.scores
<DEBUG> no gradient to module.layer2.1.conv1.weight
<DEBUG> gradient to module.layer2.1.conv1.scores
<DEBUG> no gradient to module.layer2.1.conv2.weight
<DEBUG> gradient to module.layer2.1.conv2.scores
<DEBUG> no gradient to module.layer3.0.conv1.weight
<DEBUG> gradient to module.layer3.0.conv1.scores
<DEBUG> no gradient to module.layer3.0.conv2.weight
<DEBUG> gradient to module.layer3.0.conv2.scores
<DEBUG> no gradient to module.layer3.0.shortcut.0.weight
<DEBUG> gradient to module.layer3.0.shortcut.0.scores
<DEBUG> no gradient to module.layer3.1.conv1.weight
<DEBUG> gradient to module.layer3.1.conv1.scores
<DEBUG> no gradient to module.layer3.1.conv2.weight
<DEBUG> gradient to module.layer3.1.conv2.scores
<DEBUG> no gradient to module.layer4.0.conv1.weight
<DEBUG> gradient to module.layer4.0.conv1.scores
<DEBUG> no gradient to module.layer4.0.conv2.weight
<DEBUG> gradient to module.layer4.0.conv2.scores
<DEBUG> no gradient to module.layer4.0.shortcut.0.weight
<DEBUG> gradient to module.layer4.0.shortcut.0.scores
<DEBUG> no gradient to module.layer4.1.conv1.weight
<DEBUG> gradient to module.layer4.1.conv1.scores
<DEBUG> no gradient to module.layer4.1.conv2.weight
<DEBUG> gradient to module.layer4.1.conv2.scores
<DEBUG> no gradient to module.fc.weight
<DEBUG> gradient to module.fc.scores
=> Getting CIFAR10 dataset
Files already downloaded and verified
Files already downloaded and verified
  0%|          | 0/98 [00:00<?, ?it/s]                                        0%|          | 0/98 [00:02<?, ?it/s]  1%|1         | 1/98 [00:02<04:36,  2.85s/it]  2%|2         | 2/98 [00:03<02:29,  1.56s/it]  3%|3         | 3/98 [00:04<01:48,  1.14s/it]  4%|4         | 4/98 [00:04<01:28,  1.06it/s]  5%|5         | 5/98 [00:05<01:17,  1.19it/s]  6%|6         | 6/98 [00:06<01:11,  1.29it/s]  7%|7         | 7/98 [00:06<01:07,  1.35it/s]  8%|8         | 8/98 [00:07<01:04,  1.39it/s]  9%|9         | 9/98 [00:07<00:58,  1.51it/s] 10%|#         | 10/98 [00:08<00:54,  1.63it/s]                                                10%|#         | 10/98 [00:09<00:54,  1.63it/s] 11%|#1        | 11/98 [00:09<00:50,  1.71it/s] 12%|#2        | 12/98 [00:09<00:48,  1.78it/s] 13%|#3        | 13/98 [00:10<00:46,  1.83it/s] 14%|#4        | 14/98 [00:10<00:45,  1.86it/s] 15%|#5        | 15/98 [00:11<00:44,  1.88it/s] 16%|#6        | 16/98 [00:11<00:43,  1.90it/s] 17%|#7        | 17/98 [00:12<00:42,  1.92it/s] 18%|#8        | 18/98 [00:12<00:41,  1.92it/s] 19%|#9        | 19/98 [00:13<00:41,  1.91it/s] 20%|##        | 20/98 [00:13<00:41,  1.90it/s]                                                20%|##        | 20/98 [00:14<00:41,  1.90it/s] 21%|##1       | 21/98 [00:14<00:46,  1.67it/s] 22%|##2       | 22/98 [00:15<00:47,  1.61it/s] 23%|##3       | 23/98 [00:15<00:47,  1.57it/s] 24%|##4       | 24/98 [00:16<00:47,  1.55it/s] 26%|##5       | 25/98 [00:17<00:47,  1.52it/s] 27%|##6       | 26/98 [00:17<00:47,  1.53it/s] 28%|##7       | 27/98 [00:18<00:46,  1.54it/s] 29%|##8       | 28/98 [00:18<00:42,  1.63it/s] 30%|##9       | 29/98 [00:19<00:40,  1.71it/s] 31%|###       | 30/98 [00:19<00:38,  1.77it/s]                                                31%|###       | 30/98 [00:20<00:38,  1.77it/s] 32%|###1      | 31/98 [00:20<00:36,  1.82it/s] 33%|###2      | 32/98 [00:21<00:35,  1.85it/s] 34%|###3      | 33/98 [00:21<00:34,  1.87it/s] 35%|###4      | 34/98 [00:22<00:33,  1.90it/s] 36%|###5      | 35/98 [00:22<00:33,  1.90it/s] 37%|###6      | 36/98 [00:23<00:32,  1.91it/s] 38%|###7      | 37/98 [00:23<00:31,  1.92it/s] 39%|###8      | 38/98 [00:24<00:31,  1.93it/s] 40%|###9      | 39/98 [00:24<00:30,  1.93it/s] 41%|####      | 40/98 [00:25<00:29,  1.94it/s]                                                41%|####      | 40/98 [00:25<00:29,  1.94it/s] 42%|####1     | 41/98 [00:25<00:32,  1.78it/s] 43%|####2     | 42/98 [00:26<00:31,  1.80it/s] 44%|####3     | 43/98 [00:26<00:29,  1.84it/s] 45%|####4     | 44/98 [00:27<00:28,  1.87it/s] 46%|####5     | 45/98 [00:27<00:28,  1.89it/s] 47%|####6     | 46/98 [00:28<00:27,  1.91it/s] 48%|####7     | 47/98 [00:28<00:26,  1.92it/s] 49%|####8     | 48/98 [00:29<00:25,  1.93it/s] 50%|#####     | 49/98 [00:29<00:25,  1.91it/s] 51%|#####1    | 50/98 [00:30<00:25,  1.91it/s]                                                51%|#####1    | 50/98 [00:31<00:25,  1.91it/s] 52%|#####2    | 51/98 [00:31<00:24,  1.91it/s] 53%|#####3    | 52/98 [00:31<00:23,  1.93it/s] 54%|#####4    | 53/98 [00:32<00:23,  1.94it/s] 55%|#####5    | 54/98 [00:32<00:22,  1.95it/s] 56%|#####6    | 55/98 [00:33<00:22,  1.95it/s] 57%|#####7    | 56/98 [00:33<00:21,  1.93it/s] 58%|#####8    | 57/98 [00:34<00:23,  1.76it/s] 59%|#####9    | 58/98 [00:34<00:23,  1.68it/s] 60%|######    | 59/98 [00:35<00:24,  1.62it/s] 61%|######1   | 60/98 [00:36<00:25,  1.51it/s]                                                61%|######1   | 60/98 [00:37<00:25,  1.51it/s] 62%|######2   | 61/98 [00:37<00:24,  1.51it/s] 63%|######3   | 62/98 [00:37<00:25,  1.41it/s] 64%|######4   | 63/98 [00:38<00:24,  1.42it/s] 65%|######5   | 64/98 [00:39<00:25,  1.35it/s] 66%|######6   | 65/98 [00:40<00:23,  1.39it/s] 67%|######7   | 66/98 [00:40<00:25,  1.28it/s] 68%|######8   | 67/98 [00:41<00:23,  1.33it/s] 69%|######9   | 68/98 [00:42<00:23,  1.29it/s] 70%|#######   | 69/98 [00:43<00:21,  1.34it/s] 71%|#######1  | 70/98 [00:43<00:19,  1.44it/s]                                                71%|#######1  | 70/98 [00:44<00:19,  1.44it/s] 72%|#######2  | 71/98 [00:44<00:17,  1.56it/s] 73%|#######3  | 72/98 [00:44<00:15,  1.65it/s] 74%|#######4  | 73/98 [00:45<00:14,  1.74it/s] 76%|#######5  | 74/98 [00:45<00:13,  1.80it/s] 77%|#######6  | 75/98 [00:46<00:12,  1.84it/s] 78%|#######7  | 76/98 [00:46<00:11,  1.88it/s] 79%|#######8  | 77/98 [00:47<00:11,  1.89it/s] 80%|#######9  | 78/98 [00:47<00:10,  1.90it/s] 81%|########  | 79/98 [00:48<00:10,  1.76it/s] 82%|########1 | 80/98 [00:49<00:09,  1.81it/s]                                                82%|########1 | 80/98 [00:49<00:09,  1.81it/s] 83%|########2 | 81/98 [00:49<00:09,  1.85it/s] 84%|########3 | 82/98 [00:50<00:08,  1.82it/s] 85%|########4 | 83/98 [00:50<00:08,  1.86it/s] 86%|########5 | 84/98 [00:51<00:07,  1.88it/s] 87%|########6 | 85/98 [00:51<00:06,  1.89it/s] 88%|########7 | 86/98 [00:52<00:06,  1.91it/s] 89%|########8 | 87/98 [00:52<00:05,  1.93it/s] 90%|########9 | 88/98 [00:53<00:05,  1.93it/s] 91%|######### | 89/98 [00:53<00:04,  1.93it/s] 92%|#########1| 90/98 [00:54<00:04,  1.93it/s]                                                92%|#########1| 90/98 [00:54<00:04,  1.93it/s] 93%|#########2| 91/98 [00:54<00:03,  1.92it/s] 94%|#########3| 92/98 [00:55<00:03,  1.93it/s] 95%|#########4| 93/98 [00:55<00:02,  1.94it/s] 96%|#########5| 94/98 [00:56<00:02,  1.94it/s] 97%|#########6| 95/98 [00:56<00:01,  1.96it/s] 98%|#########7| 96/98 [00:57<00:01,  1.97it/s] 99%|#########8| 97/98 [00:57<00:00,  1.99it/s]100%|##########| 98/98 [00:59<00:00,  1.24it/s]100%|##########| 98/98 [00:59<00:00,  1.65it/s]Epoch: [0][ 0/98]	Time  3.265 ( 3.265)	Data  1.227 ( 1.227)	Loss 2.449 (2.449)	Acc@1  10.74 ( 10.74)	Acc@5  49.41 ( 49.41)
Epoch: [0][10/98]	Time  0.520 ( 0.856)	Data  0.002 ( 0.113)	Loss 2.370 (2.421)	Acc@1   8.20 (  9.14)	Acc@5  49.02 ( 49.57)
Epoch: [0][20/98]	Time  0.769 ( 0.707)	Data  0.001 ( 0.060)	Loss 2.313 (2.375)	Acc@1  12.50 (  9.90)	Acc@5  51.76 ( 50.75)
Epoch: [0][30/98]	Time  0.516 ( 0.675)	Data  0.001 ( 0.041)	Loss 2.307 (2.354)	Acc@1  13.09 ( 10.53)	Acc@5  55.86 ( 51.67)
Epoch: [0][40/98]	Time  0.673 ( 0.640)	Data  0.002 ( 0.032)	Loss 2.250 (2.333)	Acc@1  14.06 ( 11.66)	Acc@5  61.52 ( 53.52)
Epoch: [0][50/98]	Time  0.520 ( 0.616)	Data  0.001 ( 0.026)	Loss 2.226 (2.316)	Acc@1  17.97 ( 12.48)	Acc@5  66.41 ( 55.21)
Epoch: [0][60/98]	Time  0.663 ( 0.614)	Data  0.002 ( 0.022)	Loss 2.214 (2.301)	Acc@1  16.80 ( 13.13)	Acc@5  64.45 ( 56.92)
Epoch: [0][70/98]	Time  0.520 ( 0.629)	Data  0.001 ( 0.019)	Loss 2.164 (2.287)	Acc@1  20.70 ( 13.78)	Acc@5  72.07 ( 58.51)
Epoch: [0][80/98]	Time  0.513 ( 0.617)	Data  0.001 ( 0.017)	Loss 2.190 (2.275)	Acc@1  16.80 ( 14.27)	Acc@5  68.55 ( 59.82)
Epoch: [0][90/98]	Time  0.522 ( 0.606)	Data  0.001 ( 0.015)	Loss 2.138 (2.264)	Acc@1  22.46 ( 14.92)	Acc@5  72.27 ( 61.00)

  0%|          | 0/20 [00:00<?, ?it/s]                                        0%|          | 0/20 [00:00<?, ?it/s]  5%|5         | 1/20 [00:00<00:11,  1.61it/s] 10%|#         | 2/20 [00:01<00:08,  2.03it/s] 15%|#5        | 3/20 [00:01<00:07,  2.21it/s] 20%|##        | 4/20 [00:01<00:06,  2.32it/s] 25%|##5       | 5/20 [00:02<00:06,  2.38it/s] 30%|###       | 6/20 [00:02<00:05,  2.42it/s] 35%|###5      | 7/20 [00:03<00:05,  2.44it/s] 40%|####      | 8/20 [00:03<00:04,  2.45it/s] 45%|####5     | 9/20 [00:03<00:04,  2.45it/s] 50%|#####     | 10/20 [00:04<00:04,  2.45it/s]                                                50%|#####     | 10/20 [00:04<00:04,  2.45it/s] 55%|#####5    | 11/20 [00:04<00:03,  2.41it/s] 60%|######    | 12/20 [00:05<00:03,  2.42it/s] 65%|######5   | 13/20 [00:05<00:02,  2.42it/s] 70%|#######   | 14/20 [00:05<00:02,  2.42it/s] 75%|#######5  | 15/20 [00:06<00:02,  2.42it/s] 80%|########  | 16/20 [00:06<00:01,  2.42it/s] 85%|########5 | 17/20 [00:07<00:01,  2.43it/s] 90%|######### | 18/20 [00:07<00:00,  2.42it/s] 95%|#########5| 19/20 [00:07<00:00,  2.43it/s]100%|##########| 20/20 [00:09<00:00,  1.60it/s]100%|##########| 20/20 [00:09<00:00,  2.18it/s]Test: [ 0/20]	Time  0.912 ( 0.912)	Loss 2.106 (2.106)	Acc@1  19.53 ( 19.53)	Acc@5  76.76 ( 76.76)
Test: [10/20]	Time  0.428 ( 0.452)	Loss 2.160 (2.133)	Acc@1  22.66 ( 20.67)	Acc@5  71.68 ( 74.45)
Test: [20/20]	Time  1.116 ( 0.469)	Loss 2.118 (2.126)	Acc@1  20.59 ( 21.03)	Acc@5  75.37 ( 74.35)
==> New best, saving at runs/resnet18-usc-unsigned/test/prune_rate=0.5/14/checkpoints/model_best.pth
Overall Timing[0/1]	epoch_time 1.2701 (1.2701)	validation_time 0.1576 (0.1576)	train_time 0.9965 (0.9965)
å¼€å§‹ç”¨åˆ†æ•°æ­£åˆ™åŒ–å¾®è°ƒ
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
<DEBUG> gradient to module.conv1.weight
<DEBUG> no gradient to module.conv1.scores
<DEBUG> gradient to module.layer1.0.conv1.weight
<DEBUG> no gradient to module.layer1.0.conv1.scores
<DEBUG> gradient to module.layer1.0.conv2.weight
<DEBUG> no gradient to module.layer1.0.conv2.scores
<DEBUG> gradient to module.layer1.1.conv1.weight
<DEBUG> no gradient to module.layer1.1.conv1.scores
<DEBUG> gradient to module.layer1.1.conv2.weight
<DEBUG> no gradient to module.layer1.1.conv2.scores
<DEBUG> gradient to module.layer2.0.conv1.weight
<DEBUG> no gradient to module.layer2.0.conv1.scores
<DEBUG> gradient to module.layer2.0.conv2.weight
<DEBUG> no gradient to module.layer2.0.conv2.scores
<DEBUG> gradient to module.layer2.0.shortcut.0.weight
<DEBUG> no gradient to module.layer2.0.shortcut.0.scores
<DEBUG> gradient to module.layer2.1.conv1.weight
<DEBUG> no gradient to module.layer2.1.conv1.scores
<DEBUG> gradient to module.layer2.1.conv2.weight
<DEBUG> no gradient to module.layer2.1.conv2.scores
<DEBUG> gradient to module.layer3.0.conv1.weight
<DEBUG> no gradient to module.layer3.0.conv1.scores
<DEBUG> gradient to module.layer3.0.conv2.weight
<DEBUG> no gradient to module.layer3.0.conv2.scores
<DEBUG> gradient to module.layer3.0.shortcut.0.weight
<DEBUG> no gradient to module.layer3.0.shortcut.0.scores
<DEBUG> gradient to module.layer3.1.conv1.weight
<DEBUG> no gradient to module.layer3.1.conv1.scores
<DEBUG> gradient to module.layer3.1.conv2.weight
<DEBUG> no gradient to module.layer3.1.conv2.scores
<DEBUG> gradient to module.layer4.0.conv1.weight
<DEBUG> no gradient to module.layer4.0.conv1.scores
<DEBUG> gradient to module.layer4.0.conv2.weight
<DEBUG> no gradient to module.layer4.0.conv2.scores
<DEBUG> gradient to module.layer4.0.shortcut.0.weight
<DEBUG> no gradient to module.layer4.0.shortcut.0.scores
<DEBUG> gradient to module.layer4.1.conv1.weight
<DEBUG> no gradient to module.layer4.1.conv1.scores
<DEBUG> gradient to module.layer4.1.conv2.weight
<DEBUG> no gradient to module.layer4.1.conv2.scores
<DEBUG> gradient to module.fc.weight
<DEBUG> no gradient to module.fc.scores
=> Using trainer from trainers.default

  0%|          | 0/98 [00:00<?, ?it/s]  0%|          | 0/98 [00:00<?, ?it/s]
wandb: - 0.022 MB of 0.022 MB uploadedwandb: \ 0.022 MB of 0.151 MB uploadedwandb: | 0.034 MB of 0.151 MB uploadedwandb: / 0.151 MB of 0.151 MB uploadedwandb: 
wandb: Run history:
wandb:            epoch â–
wandb:  epoch_test_acc1 â–
wandb:  epoch_test_acc5 â–
wandb: epoch_train_acc1 â–
wandb: epoch_train_acc5 â–
wandb:  iter_train_acc1 â–‚â–â–‚â–â–â–‚â–â–‚â–ƒâ–‚â–‚â–„â–ƒâ–„â–…â–†â–„â–…â–„â–…â–…â–†â–„â–†â–…â–†â–†â–„â–„â–‡â–…â–†â–†â–ˆâ–‡â–‡â–†â–‡â–‡â–ˆ
wandb:  iter_train_acc5 â–‚â–â–ƒâ–‚â–â–â–‚â–â–ƒâ–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–„â–…â–…â–†â–…â–‡â–…â–†â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡
wandb:  iter_train_loss â–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–…â–…â–…â–…â–„â–…â–„â–„â–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–
wandb: 
wandb: Run summary:
wandb:            epoch 0
wandb:  epoch_test_acc1 21.03
wandb:  epoch_test_acc5 74.35
wandb: epoch_train_acc1 15.336
wandb: epoch_train_acc5 61.672
wandb:  iter_train_acc1 21.72619
wandb:  iter_train_acc5 69.64285
wandb:  iter_train_loss 2.14135
wandb: 
wandb: ðŸš€ View run open_finetune_1e-06_unif_0.5_CIFAR10_cResNet18_512_1_test at: https://wandb.ai/ycx/EP%E6%94%B9%E9%80%A0%E8%AE%A1%E5%88%92/runs/e1omhfek
wandb: â­ï¸ View project at: https://wandb.ai/ycx/EP%E6%94%B9%E9%80%A0%E8%AE%A1%E5%88%92
wandb: Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241014_140121-e1omhfek/logs
Traceback (most recent call last):
  File "main.py", line 700, in <module>
    main()
  File "main.py", line 52, in main
    main_worker(args)
  File "main.py", line 231, in main_worker
    finetune(model, args, data, optimizer, criterion, writer, progress_overall)
  File "main.py", line 493, in finetune
    train_acc1, train_acc5 = train(
  File "/workspace/Edge_popup/hidden-networks-master/trainers/default.py", line 62, in train
    model = multiply_scores_weights(args.scores_lambda, model)
  File "/workspace/Edge_popup/hidden-networks-master/trainers/default.py", line 184, in multiply_scores_weights
    reg += scores_lambda * (1 - scores_temp)
UnboundLocalError: local variable 'reg' referenced before assignment
